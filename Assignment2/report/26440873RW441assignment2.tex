\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{url}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{ML441 Assignment 2}

\author{\IEEEauthorblockN{RH Buhr, 26440873}
\IEEEauthorblockA{\textit{BDatSci Programme, 4th Year} \\
\textit{Stellenbosch University}\\
Stellenbosch, South Africa \\
26440873@sun.ac.za}
}

\maketitle

\begin{abstract}

This report presents a comparative analysis of the k-Nearest Neighbors (k-NN) and Decision Tree classification algorithms for predicting forest cover types using the \texttt{forestCover.csv} dataset. The primary goal is to determine which model offers superior performance when faced with the dataset's inherent data quality issues.

The investigation begins with a foundational overview of both the k-NN and classification tree models, highlighting their advantages and disadvantages in the context of this specific problem domain. The report details the critical preprocessing strategies implemented to overcome the dataset's limitations, ensuring robust model training. A systematic approach to control parameter tuning for each classifier is described, alongside the performance metric used for a fair evaluation.

The main observation from this analysis is that while both models are capable classifiers, the classification tree demonstrates a notable advantage in predictive accuracy and interpretability for this particular dataset.

This report provides a detailed walkthrough of the process followed to arrive at this conclusion, offering valuable insights in selecting between these fundamental classification models for complex applications.

\end{abstract}

\section{\textbf{Introduction}}

The task of accurately classifying forest cover types from cartographic data presents a significant challenge in environmental monitoring and resource management. This report addresses this challenge by conducting a comparative analysis of two widely-used supervised machine learning algorithms: the k-Nearest Neighbors (k-NN) classifier and the classification tree. The primary objective is to determine which of these models provides superior predictive performance and practical utility when applied to the \texttt{forestCover.csv} dataset, a dataset with numerous data quality issues and severe class imbalance.

To achieve this goal, a rigorous methodology was employed. This involved extensive data preprocessing tailored to the specific requirements of each algorithm, including strategies to handle missing values, correlated features, and outliers. The critical issue of class imbalance was addressed by applying the Synthetic Minority Oversampling Technique (SMOTE) to the training data. After the pre-processing stage a systematic hyperparameter tuning process was conducted for both models using a 5-fold cross-validation grid search, with the macro-averaged F1-score as the primary evaluation metric to ensure a fair comparison across all classes.

The analysis reveals that while both classifiers are highly effective, the classification tree demonstrates a slight, though not statistically significant, advantage in overall predictive accuracy and robustness. It achieved an accuracy of 94\% compared to the k-NN model's 93\% and showed more consistent performance on the challenging minority classes. Given its high interpretability and significantly lower pre-processing, and computational resource requirements, the classification tree is ultimately recommended as the more effective solution for this specific problem.

The remainder of this report is structured as follows: Section 2 provides a background on the machine learning algorithms and discusses their expected performance given the data quality issues. Section 3 details the implementation process. Section 4 describes the empirical process, including data pre-processing and hyperparameter tuning, as well as the analysis process used. Section 5 presents and discusses the results of the model evaluations, and Section 6 concludes the report with a final recommendation.

The complete code for this analysis can be found at \url{https://github.com/ruanbuhr/ML_Assignments/blob/main/Assignment2/assign2.ipynb}.

\section{\textbf{Background}}

The task of predicting a forest cover type from cartographic data is a classic example of a supervised learning problem, specifically a multi-class classification problem. The core idea is to train a model on a dataset containing observations (parcels of land) with known descriptive features and known response labels. The trained model should then be able to predict the correct label for new, unseen observations. Myriad machine learning algorithms exist to accomplish this task, each with unique methodologies and inductive biases. This report will focus on two distinct approaches, an information-based method and a similarity-based method. 

\subsection{\textbf{Machine learning algorithm description}}

\subsubsection{\textbf{Classification Tree}}

A classification tree is an information-based machine learning model, that forms a tree-like structure of human-readable rules.. Non-terminal nodes in this tree represent rules applied to descriptive features, and terminal nodes represent the response feature predictions. The algorithm learns to recursively split the data into increasingly pure subsets based on the feature values that provide the most information. The homogeneity (purity) measures often used by classification trees are entropy and the Gini index.

Entropy is an information theory concept that measures the level of disorder of uncertainty in a set. In the context of decision trees, it measures the impurity of a node. Entropy falls between [0, 1], and an entropy value of 0 signifies a perfectly pure node, where all observations are from the same class. An entropy value of 1 represents the hightest level of disorder, e.g. a 50/50 split in a binary classification problem. The classification tree algorithm selects the split that provides the highest information gain, which is the reduction in entropy after the split. The formula for entropy is: $Entropy = -\sum_{i=1}^{C} p_i \log p_i$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

An alternative measure is the Gini index. It is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it were randomly labeled according to the distribution of the labels in the set. The Gini index also falls in the range [0, 1], and a Gini index score of 0 represents a perfectly pure node, while a higher score indicates greater impurity. Just like entropy, the algorithm seeks to find rules that result in the largest decrease in Gini index. The formula for Gini index is: $Gini\ index = 1 - \sum_{i=1}^{C} (p_i)^2$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

To prevent overfitting, classification trees are simplified through a process called pruning, which reduces the tree's size by removing branches that contribute little to its predictive power. A tree that grows till every terminal node is perfectly pure often captures noise in the training data and generalizes poorly to unseen data. There are two main strategies to prune classification trees and achieve a better trade-off between bias and variance.

Pre-pruning stops the tree's growth before it is fully induced. This method involves setting stopping criteria that halt the splitting of nodes once they are met. Examples include:

\begin{itemize}
    \item Maximum Tree Depth: Limit longest path from root to leaf nodes.
    \item Minimum Samples for a Split: Require a node to have at least a certain amount of samples before it can be split.
    \item Minimum Samples per Leaf: Ensure that terminal nodes contain a minimum number of samples.
\end{itemize}

Pre-pruning is cost-effective because it stops the tree from becoming too large, but a potential drawback is that it might stop the growth too early. This is where post-pruning comes in. It allows the tree to grow to its maximum size and then systematically cuts back the branches that are least important. The algorithm prunes subtrees that provide weak predictive power, typically be evaluating the tree's performance on a separate validation set \cite{b1}.

Classification Trees are a good approach to solve the given problem. They are highly interpretable, require little data preprocessing, and capture non-linear relationships. However they are prone to overfitting and can create biased trees if some classes dominate.

\subsubsection{\textbf{k-Nearest Neighbors}}

k-Nearest neighbors is a similarity based machine learning model. k-NN is a non-parametric algorithm, that assumes that an observation is likely to be similar to the data points closest to it in feature space.

k-NN works by storing all training instances, when a new instance to be predicted is presented to the model the instance is compared to the training data to find the most similar instances. The label of this instance is then assigned the majority label in the set of its closest k neighbors. To find these k most similar points k-NN uses difference metrics such as Euclidean distance: $d(x_1, x_2) = \sqrt{\sum_{i=1}^{p} (x_1i - x_2i)^2}$, Manhattan distance: $d(x_1, x_2) = \sum_{i=1}^{p} |x_1i - x_2i|$, or Minkowski distance: $d(x_1, x_2) = (\sum_{i=1}^{p} |x_1i - x_2i|^q)^\frac{1}{q}$, to quantify the distance between observations, where $p$ is the number of descriptive features, and $q$ is a parameter.

Once the distances between the observation of interest and all other observations in the training set have been calculated, the algorithm can then select the k closest points (smallest distance), and assign a prediction to the label of the observation of interest based on a majority vote or a weighted majority vote.

The value of k is a hyperparameter that needs to be tuned via cross validation or another robust method. A low value for k will result in a model with high variance and low bias, potentially overfitting the training data. While a high value for k will result in a model with low variance and high bias, which will underfit the training data.

kNN is a robust and flexible non-parametric machine learning model, but it does have some drawbacks. It is extremely sensitive to feature scale. If one feature is orders of magnitude larger than the others then it will dominate in the distance calculations and the information provided by all other features would be disregarded. Hence it is always best to standardize all features before training a k-NN model.

\subsection{\textbf{Expectations wrt data quality issues}}

\subsubsection{\textbf{Classification Tree}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact classification tree models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Unlike other algorithms, that require missing values to be removed or imputed, some classification tree algorithms, like CART of C4.5, have elegant, built-in methods to handle missing values directly, \cite{b2}.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: The predictive accuracy of the model will not be affected. The tree induction algorithm will select one of the features for a split (which ever provides the most information gain), and the other will not be considered for subsequent splits as it does not provide much additional information. The only part of the model that is affected is the structure, since these two features are correlated the algorithm arbitrarily choses one of them to do the initial split on, if slightly different training data is used, the algorithm might choose the other variable.
    \item \textbf{\texttt{Inclination} contains only noisy values}:  A feature containing only noisy values will have very little negative impact on the final performance of a classification tree. Classification tree induction algorithms find the best feature and split-point that most effectively separates the data into distinct classes. A noisy feature has values that are essentially random wrt the target. This means that any split made on this feature will fail to create more informative subgroups. Since the tree building algorithm is greedy it will always look for more informative features and ignore \texttt{Inclination}. However if the noisy feature happens to be correlated with the target by chance then the algorithm might end including it in the tree leading to a more complex, overfit model.
    \item \textbf{Features with outliers}: Classification trees are robust to outliers, their structure and splitting criteria naturally isolate extreme values. Trees split based on thresholds that separate groups, a few extreme values rarely change where the best split is. Outliers get isolated into their own small leaf nodes that do not distort the main decision boundaries learned from the majority data. 
    \item \textbf{Features with differing numeric ranges}: This will have no impact on a classification tree. Classification trees are completely insensitive to the scale of the features, they only consider thresholds within features, and do not consider feature scales. Entropy and the Gini index only care about the purity of resulting groups, not the magnitude or scale of the threshold itself.
    \item \textbf{Numerical and categorical features}: Decision trees can naturally handle both numerical and categorical features. They are non-parametric models that do nto rely on distance calculations which make them uniquely suited for datasets with mixed feature types. For numerical features, the algorithm finds an optimal threshold that best separates classes. For categorical features, the algorithm partitions the categories into two or more subsets that result in the purest subgroups.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one has no impact on the final classification tree model, the algorithm completely ignores it. Since \texttt{Water\_Level} is constant, it is impossible to split the data using it. This means this feature provides no information to the model and it will not aid in reducing impurity. The greedy nature of tree-based models ensure that they will never choose features that provide zero benefit.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: A feature with a unique value for each observation will have a severely negative impact on a classification tree, causing it to overfit the training data. Since decision tree algorithms are greedy a unique identifier presents itself as the perfect feature for splitting, because every value in the \texttt{Observation\_ID} column is unique, the algorithm can create a split for each individual observation resulting in leaf nodes each containing one data point. This will minimize impurity to the greatest extent, and the algorithm sees this as the ultimate success. Features with unique value for each observation will cause classification trees to severely overfit the training data, having 100\% train accuracy, but very poor generalization.
    \item \textbf{Skew class distribution}: Training a classification tree on a dataset with an extremely skewed response will create a biased model that performs poorly on the minority class. The easiest way for the tree to achieve high accuracy is to simply predict the majority class most of the time. Performance metrics like accuracy will show good performance, but minority class recall will be very poor.
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact k-NN classification models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values directly disrupt the k-NN algorithm because it relies on distances between data points to make predictions. If a feature's value is missing the distance calculation cannot be completed and the model will fail.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: Correlated features can severely bias a k-NN model by giving disproportionate weight to the single underlying trait they represent, effectively counting its influence twice during the distance calculation. This distortion of feature space skews the identification of nearest neighbors, which can negatively impact the performance of the model.
    \item \textbf{\texttt{Inclination} contains only noisy values}: A feature containing only noisy values will negatively impact a k-NN classification model. Noisy features introduce randomness and irrelevant information, which can distort distance calculations making the model less accurate. The noisy feature adds random, meaningless fluctuations to the computed distances. This distortion might lead the algorithm to identify incorrect nearest neighbors, choosing neighbors closer in terms of the noisy feature, but farther away in terms of the meaningful features. Additionally including a noisy feature will lead to overfitting, since the model might try to learn the noise, which will lead to poor generalization.
    \item \textbf{Features with outliers}: Outliers can negatively impact a k-NN classifier because its distance calculation is highly sensitive to the magnitude of feature values. For instance, two data points might be very close to each other in several descriptive features, but an outlier in a single feature can create such a large distance in that one dimension that it disproportionately inflates the overall distance metric. This skew causes the algorithm to misidentify the true nearest neighbors, leading to inaccurate predictions.
    \item \textbf{Features with differing numeric ranges}: Differing feature magnitudes have significant impact on k-NN based classification models. Features with larger numeric ranges will disproportionately dominate the distance calculations, overshadowing the contributions of features on smaller scales, regardless of predictive importance. This bias can severely skew the identification of nearest neighbors, leading to a reduction in the model's performance
    \item \textbf{Numerical and categorical features}: A k-NN classifier cannot be directly applied to a dataset with a mix of numerical and categorical features because its distance-based metrics are only applicable to numeric features, and undefined for categorical features. It is critical to preprocess the categorical features correctly to transform them into a numerical format. Care must be taken to distinguish between nominal and ordinal features as incorrect encoding can create artificial relationships that mislead the model.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one will have no impact on a k-NN classification model, because it provides no useful information to distinguish between data points. When the k-NN algorithm calculates distance between any two points, the difference between the \texttt{Water\_Level} feature between both points will be zero. It contributes nothing to the overall distance calculation and has no influence on the nearest neighbors selected.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Including a unique identifier is highly detrimental to a k-NN model, as it introduces data with no predictive value that can dominate the distance metric. This can cause the model to overfit by trying to learn arbitrary relationships between data points base on the observation ID rather than genuine patterns in the descriptive features, leading to negatively impacted model performance and poor generalization.
    \item \textbf{Skew class distribution}: A skewed class distribution significantly degrades k-NN classifier performance, as the algorithm's majority voting mechanism is inherently biased towards the dominant class. When one or two classes vastly outnumber the others, their distances are more likely to dominate any given neighborhood. Consequently the model will frequently misclassify minority class instances as the majority classes, leading to high accuracy for the majority class, but low recall for the minority class. This predictive bias becomes even more pronounced as the value of \textit{k} increases, as a larger neighborhood is more likely to reflect the imbalanced distribution.
\end{itemize}


\section{\textbf{Implementation}}

The primary goal is to classify forest cover types by implementing, tuning, and comparing two distinct classification algorithms, decision tree and k-NN classifiers.

To achieve this goal, Python with Pandas, and Scikit-Learn was used to preprocess the given \texttt{forestCover.csv} dataset and train both a classification tree and a k-NN classifier on the cleaned data.

The stages of the implementation process for each model consisted of: 

\begin{itemize}
    \item Data preprocessing
    \item Addressing class imbalance in training set
    \item Systematic hyperparameter tuning using grid search with cross-validation
    \item Final model evaluation on unseen test data
\end{itemize}

During the data preprocessing stage, the data was prepared differently for the decision tree than the k-NN model, this required the use of two different copies of the original dataset. Since the decision tree is more robust to the issues that are mentioned to still be present in the dataset, less preprocessing was required. The dataset to be used for k-NN model training required substantially more preprocessing as k-NN models, being distance based models, are more sensitive to the issues present in the dataset.

After the data quality issues in the descriptive features for both datasets were handled accordingly, the class imbalance in the response, \texttt{Cover\_Type}, was addressed by applying SMOTE. This balanced the dataset by over sampling the minority classes (3, 4, 5, 6, 7) to create a more balanced dataset. It is important to note that SMOTE was applied only to the training datasets after both datasets were were train-test split to prevent data leakage and ensure unbiased evaluation of the model's performance on unseen data.

After applying SMOTE, the models were trained and hyperparameter tuning was done. For both models grid search with 5-fold cross validation was used. Grid search is a hyper parameter tuning approach where a user provides a set of all values the different model parameters are allowed to take on, the algorithm then evaluates and compares all the different combinations of hyperparameter values in order to find the best one. 5-Fold cross validation was used because it strikes a perfect balance between speed and stability.

Performance metric used to score the models during cross validation was F1 Macro. The macro-averaged F1 score calculates the F1 score for each class independently and then finds their unweighted average. This means that it treats all classes as equally important, regardless of how many samples are in the dataset. Even though SMOTE forces a model to pay attention to underrepresented classes during training the goal is not just to have a balanced training set, it is to build a model that performs well on the real-world imbalanced data. Using F1 Macro as scoring metric causes the cross validation algorithm to consider all categories equally important, including those that were originally in the minority.

After cross validation yielded the best classification tree and k-NN classifier, the models were evaluated by analyzing their accuracy, precision, recall, and F1 score which is critical for understanding model performance in a multi-class problem.

\section{\textbf{Empirical process}}

\subsection{\textbf{Data pre-processing}}

\subsubsection{\textbf{Classification Tree}}

This subsection details the data pre-processing steps that were taken to train the classification tree model. The following list mentions the data quality issues and the preprocessing steps followed:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values were dropped. The observations with missing values were found to make up only 0.05\% of the dataset, meaning removing them will not bias the dataset. Even though some classification tree algorithms are robust to missing values, SMOTE is not, and as discussed later, it is critical that SMOTE be applied to the training dataset.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: Nothing was done. As mentioned previously, correlated features do not have any effect on decision trees, because the information gain by the underlying feature is only captured in one split. The other variable will not provide any additional information and will not be considered for future splits.
    \item \textbf{\texttt{Inclination} contains noisy values}: Nothing was done. Classification tree induction splits data based on information gain. A noisy feature provides no information gain, hence the algorithm will just ignore features that contain noisy values.
    \item \textbf{Features with outliers}: Nothing was done. Outliers do not affect the thresholds decided upon by decision trees, because a few extreme values rarely change where the best split is.
    \item \textbf{Features with differing numeric ranges}: Nothing was done as classification trees are completely insensitive to the scale of features since they only consider splits/thresholds within features. The purity metrics only care about the purity of resulting subgroups, not the magnitude of the values.
    \item \textbf{Numerical and categorical features}: Convert categorical feature to numeric. Even though classification trees can naturally handle numeric and categorical features, SMOTE can not, hence the only categorical feature \texttt{Soil\_Type1} needed to be converted to numeric. All occurrences of \textit{positive} were assigned a value of \textit{0}, and all occurrences of \textit{negative} were assigned a value of \textit{1}. This keeps the \texttt{Soil\_Type} one-hot encoded group mutually exclusive.
    \item \textbf{\texttt{Water\_Level} has cardinality of one}: Nothing was done. Constant features are ignored by the tree induction algorithm. Since the feature is constant, there is no way for the algorithm to split the data using this feature, hence it provides no information gain and is just ignored.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Removed \texttt{Observation\_ID} as a descriptive feature. Features with unique values for each observation are detrimental to the performance of decision trees, as they provide maximal information gain to split on. Splitting on these features will cause each observation to be in it's own leaf node, leading to severe overfitting. To combat this, \texttt{Observation\_ID} was removed as a descriptive feature. When using pandas to load the dataset, \texttt{df = pd.read\_csv('data/forestCover.csv', index\_col='Observation\_ID', na\_values='?')}, \texttt{Observation\_ID} is set to the index to prevent it from being used as a descriptive feature.
    \item \textbf{Skew class distribution}: SMOTE was applied to the training dataset to oversample the minority classes and create synthetic observations so the model can more precisely learn the patterns present in the minority classes, instead of the majority classes dominating. 
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors}}

This subsection details the data pre-processing steps that were taken to train the k-NN classifier model. The following list mentions the data quality issues and the preprocessing steps followed:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values were dropped. As mentioned above the observations with missing values were found to make up only 0.05\% of the observations, meaning removing them will not bias the dataset. k-NN is a distance based algorithm, its distance calculations are only defined for numeric values and will not work for missing values. Also SMOTE is not robust to missing values. 
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: \texttt{Aspect} was dropped from the dataset. Correlated features can severely distort k-NN ability to find true nearest neighbors, because the underlying effect is counted twice, which skews the k-NN distance metrics.
    \item \textbf{\texttt{Inclination} contains noisy values}: \texttt{Inclination} was dropped from the dataset. Noisy features contain random fluctuations which distort the k-NN distance metrics making them unable to find the true nearest neighbors. Removing noisy features is the best course of action.
    \item \textbf{Features with outliers}: A $log(1 + x)$ transformation was applied to the feature \texttt{Horizontal\_Distance\_To\_Hydrology} and afterward Scikit-Learn \texttt{RobustScaler} was applied to all features. The $log(1 + x)$ was applied to \texttt{Horizontal\_Distance\_To\_Hydrology}, because it is the only feature with outliers that severely skew its distribution. It was decided to keep these outliers, because they represent observations of forest data that are not close to a water source in horizontal distance and removing them would lead to a loss in information. The $log(1 + x)$ transformation keeps all the information in the feature while drastically reducing the distance between inlier and outlier data points. After the transformation was applied Scikit-Learn \texttt{RobustScaler} was applied to all features. It scales features using statistics that are immune to outliers, like the median and interquartile range. This keeps all information present in the features while reducing the effects of outliers.
    \item \textbf{Features with differing numeric ranges}: Scikit-Learn \texttt{RobustScaler} was applied to all features. Differing numerical ranges can greatly skew a k-NN models ability to find the true nearest neighbors, as features with larger magnitudes dominate over smaller features in the distance calculations. The \texttt{RobustScaler} was used because, as discussed above, it scales data in a way that mitigates the effect of outliers while ensuring every feature contributes fairly to the distance metric.
    \item \textbf{Numerical and categorical features}: Convert categorical features into numeric. k-NN distance metrics are only defined for numerical data and do not work on categorical data, so categorical features need to be converted to numerical features. The only categorical feature \texttt{Soil\_Type1} needed to be converted to numeric. All occurrences of \textit{positive} were assigned a value of \textit{0}, and all occurrences of \textit{negative} were assigned a value of \textit{1}. This ensures k-NN distance metrics are defined for this feature.
    \item \textbf{\texttt{Water\_Level} has cardinality of one}: Nothing was done. When k-NN calculates distance between two points, the distance between the \texttt{Water\_Level} component will always be zero because it is a constant feature, this means it has no effect on the model's ability to find the nearest neighbors.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Removed \texttt[Observation\_ID] as a descriptive feature. Including a feature with a unique value for each observation provides no predictive information while adding data that can skew the distance metric and interfere with a k-NN model's ability to find nearest neighbors. To combat this, \texttt{Observation\_ID} was removed as a descriptive feature. When using pandas to load the dataset, \texttt{df = pd.read\_csv('data/forestCover.csv', index\_col='Observation\_ID', na\_values='?')}, \texttt{Observation\_ID} is set to the index to prevent it from being used as a descriptive feature.
    \item \textbf{Skew class distribution}: SMOTE was applied to the training dataset to oversample the minority classes and create synthetic observations so the model can more precisely learn the patterns present in the minority classes, instead of the majority classes dominating. 
\end{itemize}

\subsection{\textbf{Control parameter tuning}}

To optimize the predictive performance of the models and prevent overfitting, hyperparameter tuning was done using Scikit-Learn's \texttt{GridSearchCV}. This method exhaustively searches for the optimal combination of hyperparameter values from a predefined grid of values. The search was guided by 5-fold cross-validation and the macro-averaged F1-score was used as the evaluation metric.

5-Fold cross validation was used, because it strikes a perfect balance between speed and stability. It works by splitting the dataset into five equal parts, and for each round (five total) the algorithm uses four folds for training and one for testing. The test fold is rotated each round, and the five test scores are averaged in the end to get a final model performance estimate.

\subsubsection{\textbf{Classification Tree}}

For the decision tree classifier, the tuning process focussed on controlling the model's complexity to find the perfect balance between bias and variance. The following hyperparameters were explored:

\begin{itemize}
    \item \texttt{criterion}: Impurity measure: [Gini-index, Entropy]
    \item \texttt{max\_depth}: Maximum depth of the tree: [10, 20, 30, 40, 50]
    \item \texttt{min\_samples\_split}: The minimum samples required to split an internal node: [2, 5, 10].
    \item \texttt{min\_samples\_leaf}: The minimum samples required to be at a leaf node: [1, 2, 4].
\end{itemize}

The grid search identified the optimal hyperparameters for the classification tree, which achieved a mean 5-fold cross-validation F1 macro score of 0.9614. The best parameter set was found to be:

\begin{itemize}
    \item \texttt{criterion}: Entropy
    \item \texttt{max\_depth}: 40
    \item \texttt{min\_samples\_split}: 2
    \item \texttt{min\_samples\_leaf}: 1
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors}}

The performance of the k-NN classifier is highly dependent on the choice of neighbors, distance metric, and distance weighting scheme. Given the computational expense of k-NN on large datasets the hyperparameter search was conducted on a random subsample of 200000 instances (after SMOTE the dataset had 1500000 observations).

The grid search explored the following parameters:

\begin{itemize}
    \item \texttt{n\_neighbors}: The number of neighbors: [3, 5, 6, 9, 11]. Odd values were used to avoid ties.
    \item \texttt{weights}: Weighting scheme used when considering nearest neighbors to make a prediction. Values tested were \texttt{uniform}, where all points are weighted equally, and \texttt{distance}, where closer neighbors have a greater influence than neighbors further away.
    \item \texttt{metric}: Distance metric used: [Euclidean, Manhattan].
\end{itemize}

The grid search identified the optimal hyperparameters for the k-NN classifier which achieved a mean 5-fold cross-validation F1 macro score of 0.9379. The optimal hyperparameters were:

\begin{itemize}
    \item \texttt{n\_neighbors}: 3
    \item \texttt{weights}: Distance
    \item \texttt{metric}: Manhattan
\end{itemize}

\subsection{\textbf{Performance metric}}

To evaluate the performance of the classification models, macro averaged F1-score was used as the primary metric. This choice was made to address the specific characteristics of the \texttt{forestCover} dataset, namely its multi-class nature and significant class imbalance.

The F1-score is the harmonic mean of precision and recall, providing a single score that balances both concerns. Precision measures the accuracy of positive predictions, while recall measures the model's ability to find all relevant instances of a class.

$$
F1 = 2 \times \frac{precision \times recall}{precision + recall}
$$

For a multi-class problem, like this one with 7 \texttt{Cover\_Type} categories, macro-averaging the F1-scores across all classes calculates the F1-score independently for each of the 7 classes then then computes their unweighted average.

It is superior to other common metrics, like accuracy, for this problem, for several reasons:

\begin{itemize}
    \item Handles class imbalance effectively: Even though the imbalance was corrected with SMOTE on the training set, the model could still achieve high accuracy on unseen, imbalanced data.
    \item Provides balanced view: The use of the harmonic mean ensures that a model must perform well on both precision and recall to get a high score.
\end{itemize}

In summary, the macro-averaged F1-score provides the most robust and fair evaluation of model performance for this inherently imbalanced, multi-class classification task.

\subsection{\textbf{Analysis process}}

After optimal hyperparameters were found, final classification tree and k-NN classifier models were trained using these parameters and the full training datasets. Afterward analysis was conducted to compare the predictive performance of the two classifiers on the \texttt{forestCover} dataset.

To ensure robust comparison between the classification tree and k-NN models the analysis employed a multi-faceted evaluation framework. This involved assessing these models against a suite of performance metrics designed to capture different aspects of predictive power, followed by a formal statistical test to determine the significance of any performance differences.

Simply looking at one number can be misleading, especially for multi-class problems. Therefore several metrics were used to create a holistic view of each model's performance.

\textbf{Accuracy} is represents the proportion of correctly classified observations. It provides a general assessment of the model's performance. However, it can be deceptive on datasets with significant class imbalance. Since SMOTE was used to remedy the class imbalance, the accuracy metric for these two models are quire reliable, but additional metrics were also looked at.

\textbf{Precision, Recall, and F1-score} provide deeper insights into the types of errors a model makes. Precision measures how accurate a model's positive predictions are. Recall measures how well a model identifies all positive instances. F1-score is the harmonic mean of precision and recall that ensures a model balances both metrics. If a model has good values for all these metrics, it indicates that the model is both accurate in its positive predictions and effective at capturing most of the actual positive instances, demonstrating strong overall performance with minimal trade-offs between false positives and false negatives.

\textbf{Cohen's Kappa} measures the agreement between the model's predictions and the actual labels, while correcting for the probability of agreement occurring by chance. A Kappa score of 1 indicates perfect agreement, while a Kappa score of 0 indicates that the model performance is no better than random guessing. Negative values indicate that the model performs worse than random guessing.

\textbf{Matthews Correlation Coefficient (MCC)} is one of the most reliable evaluation metrics for classification. It is a correlation coefficient between the observed and predicted classifications. Its values range from -1 to +1. A score of +1 indicates perfect prediction, 0 indicates a prediction no better than random guessing, and -1 indicates total disagreement between prediction and observation.

Additionally a \textbf{Wilcoxon Signed-Rank Test} was done to compare the difference between these two models. The Wilcoxon test is a non-parametric statistical hypothesis test used to compare two related samples.

The test is designed for paired data. n this analysis, the data pairs are the accuracy scores of the classification tree and the k-NN model on each of the 10 identical cross-validation folds. Evaluating both models on the same sub-datasets ensures a direct, fair comparison.

Unlike a paired t-test, it does not assume the differences in scores are normally distributed. This makes it a safer and more robust choice for comparing model performance scores, which often don't follow a normal distribution.

The test is structured around a null and alternative hypothesis:

\begin{itemize}
    \item \textbf{Null Hypothesis $H_0$}: There is no statistically significant difference between the performance of the two models.
    \item \textbf{Alternative Hypothesis $H_A$}: There is a statistically significant difference between the models' performance.
\end{itemize}

The test calculates the difference between each pair of accuracy scores. It then ranks these differences by absolute magnitude and sums the ranks of the positive and negative differences separately.

This process yields a p-value, which represents the probability of observing the performance difference seen in the sample if the null hypothesis is true. A small p-value ($p < 0.05$) provides strong evidence against the null hypothesis. It suggests the difference between the models is unlikely to be due to random chance, meaning the null hypothesis is rejected. The opposite is true for large p-values.

\section{\textbf{Results \& discussion}}

This section presents discussions and interpretations of the metrics and tests used to analyze the classification tree and k-NN classifier models.

\subsection{\textbf{Classification Tree}}

The confusion matrix in figure~\ref{fig:cm_tree} clearly shows that the final classification tree model trained performs well on the test data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/cm_tree.pdf}
    \caption{Confusion Matrix of Classification Tree Predictions}
    \label{fig:cm_tree}
\end{figure}

The model's predictions are strong, indicated by the high values on along the main diagonal. These values represent the correctly classified instances for each class. The confusion matrix shows a visual result of how the classification tree model performs, but accompanied by numerical measures, it tells the full story.

\begin{table}[H]
    \centering
    \caption{Classification Report for Decision Tree Model}
    \label{tab:class_rep_tree}
    \begin{tabular}{cccccc}
        \toprule
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
        \midrule
        1 & 0.94 & 0.94 & 0.94 & 42349 \\
        2 & 0.95 & 0.94 & 0.95 & 56629 \\
        3 & 0.91 & 0.92 & 0.92 & 7147 \\
        4 & 0.81 & 0.85 & 0.83 & 549 \\
        5 & 0.75 & 0.85 & 0.80 & 1898 \\
        6 & 0.84 & 0.87 & 0.86 & 3471 \\
        7 & 0.93 & 0.96 & 0.95 & 4100 \\
        \midrule
        \textbf{Accuracy} &  &  & 0.94 & 116143 \\
        \textbf{Macro avg} & 0.88 & 0.91 & 0.89 & 116143 \\
        \textbf{Weighted avg} & 0.94 & 0.94 & 0.94 & 116143 \\
        \bottomrule
    \end{tabular}
\end{table}

The classification report in table~\ref{tab:class_rep_tree} provides an even clearer representation of the model's performance. The classification tree model demonstrates strong overall performance, with an accuracy of 94\%. This means 94\% of the predictions made by the model were correct. The weighted average for precision, recall, and F1-score are all 0.94, which shows that the model performs consistently well across all classes.

However the macro average of precision, recall, and F1-score are slightly lower than the weighted average. This is important, because the macro average treats all classes equal regardless of their size. The difference between the weighted and macro averages indicates that the model's performance on the smaller classes is slightly worse than its performance on the larger classes, which can clearly be seen in figure~\ref{fig:cm_tree}.

Classes 1, and 2 are the largest classes, indicated by their support values of 42349 and 56629 respectively (it is important to note that the testing dataset was not preprocessed in any way to make it balanced). The model performs exceptionally well on these classes, with precision, recall, and F1-scores all above 0.93. Even though the training data was balanced with the use of SMOTE, the model still performs best when predicting the majority classes, this is because it had plenty of authentic data to learn from, where the other classes had mostly synthetic data.

Class 7 is of moderate size in the test set (4100), but the model still performs well when predicting this class even though most of the data learnt from was synthetic.

Class 3 is also of moderate size and the model's performance still remains strong when predicting instances of class 3. The precision, recall, and F1-score only have slightly lower values for class 3 than the majority classes.

Classes 4, 5, and 6 are the most challenging classes for the model to predict. They are minority classes, with the smallest number of instances which means that most of the data of these classes that the model learned from was probably synthetic. Even so the performance of classes 4, and 6 are not horrendous, the metrics are only slightly lower than class 3. Class 5 is the class the model struggled with the most. The model achieves a precision of 0.75 for class 5 instances which means that of all the samples that the model predicted as class 5, only 75\% were actually class 5. This is not terrible, but it can be better.

Overall the classification report and confusion matrix support the idea that the classification tree is a strong classifier with high overall accuracy. However the model does seem to perform noticeably better on the majority classes compared to the minority classes even though SMOTE was used to address the imbalance.

The classification tree has a \textbf{Cohen's Kappa} value of 0.8996, which indicates that the model has a very strong agreement with the true labels, well beyond random chance. The overall accuracy the model achieved of 94\% can be misleading since the dataset is highly imbalanced. If a model has a high accuracy but a very low Kappa score it means that most of that accuracy could be due to chance agreement rather than true predictive power, which usually happens if a dataset is imbalanced. The high Kappa value of 0.8996 indicates that the model's predictions are reliable and that the model in fact has learned to classify the majority and minority classes effectively.

Further, the classification tree has a \textbf{Matthews Correlation Coefficient (MCC)} of 0.8996, which also indicates that it is a strong and reliable model. An MCC value of 0.8996 indicates that the model's predictions are highly correlated with the true outcomes. This value is very close to the perfect outcome, 1, which means the model is highly effective at correctly identifying both positive and negative instances on a per class level. Again, unlike accuracy which can be misleading on imbalanced datasets, MCC provides a balanced measure because it concerns both types of errors (false positives, and false negatives) as well as correct classifications, MCC balances performance across all classes. The model's MCC value of 0.8996 further confirms that it is a strong and reliable classifier. It also provides evidence that the model's performance is not the result of chance or bias toward a particular class, but rather a reflection of the model's learned insights from the data.

\subsection{\textbf{k-Nearest Neighbors}}

The confusion matrix in figure~\ref{fig:cm_knn} shows that the k-NN classifier model performs well on the test data. In fact for most of the minority classes it performs better than the classification tree.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/cm_knn.pdf}
    \caption{Confusion Matrix of k-NN Classifier Predictions}
    \label{fig:cm_knn}
\end{figure}

The confusion matrix indicates that the model's predictions are strong, which is evident by the high values on along the diagonal.

\begin{table}[H]
    \centering
    \caption{Classification Report for k-NN Model}
    \label{tab:class_rep_knn}
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
        \midrule
        1 & 0.93 & 0.93 & 0.93 & 42349 \\
        2 & 0.96 & 0.93 & 0.94 & 56629 \\
        3 & 0.90 & 0.91 & 0.91 & 7147 \\
        4 & 0.78 & 0.86 & 0.82 & 549 \\
        5 & 0.67 & 0.92 & 0.77 & 1898 \\
        6 & 0.77 & 0.89 & 0.83 & 3471 \\
        7 & 0.90 & 0.98 & 0.94 & 4100 \\
        \midrule
        \textbf{Accuracy} &  &  & 0.93 & 116143 \\
        \textbf{Macro avg} & 0.84 & 0.92 & 0.88 & 116143 \\
        \textbf{Weighted avg} & 0.93 & 0.93 & 0.93 & 116143 \\
        \bottomrule
    \end{tabular}
\end{table}

The classification report in table~\ref{tab:class_rep_knn} provides metrics for further investigating the performance of the model. The k-NN model demonstrates strong overall performance, with an accuracy of 93\%. The weighted average for precision, recall, and F1-score are all 0.93, which shows that the model performs well across all classes.

The macro average of precision, recall, and F1-score are lower than the weighted average. This difference indicates that the model's performance on minority classes is slightly worse than its performance on majority classes. Which is clearly evident in figure~\ref{fig:cm_knn}.

Classes 1, and 2 are the largest classes, and it is no surprise that the model performs best on these classes. The precision, recall, and F1-score of the model for these classes are all equal or above 0.93, indicating that the model has no problem balancing precision and recall.

Class 7, even though it is a minority class, has metric values comparable to that of the majority classes. The precision, recall and F1-score are all larger than 0.9 even though the model had to learn this class from mostly synthetic data.

The model also performs well when predicting class 3, even though it is a minority class. The precision, recall, and F1-score are all equal or above 0.9.

Just like the decision tree, classes 4, 5, and 6 are the most challenging classes for the model to predict, this is expected as these are all minority classes. However what is interesting is that the k-NN model struggles more to predict these classes than the classification tree model. The values of all of these metrics are lower than those of the classification tree model, albeit not by a lot, except for the metrics related to class 5. The model has a precision of 0.67 when predicting class 5 instances which means that of all the samples that the model predicted as class 5, only 67\% were actually class 5. This is drastically worse than the classification tree. The only redeeming fact is that the recall for this class is 0.92, which means the F1-score comes out to 0.77, which is worse than the decision tree, meaning the model does not have as favorable of a balance between precision and recall as the decision tree when predicting class 5.

In summary the classification report and the confusion matrix both show that the k-NN classifier is a strong classifier with high overall accuracy, but it does perform worse than the classification tree model in nearly all evaluation metrics discussed.

The k-NN model has a \textbf{Cohen's Kappa} value of 0.8870, which indicates that the model has a very strong agreement with the true labels, just like the classification tree. As mentioned in the previous section, the accuracy of a model on an imbalanced dataset can be misleading, that is why it is always best to look at other metrics like Kappa as well. The high Kappa value of 0.8870 indicates that the model's predictions are reliable, and that the model has learned to classify the majority and minority classes effectively. Although this Kappa value is good, it is a little bit worse than the value achieved by the classification tree.

The k-NN model has a \textbf{Matthews Correlation Coefficient (MCC)} of 0.8873, which also indicates that it is a strong and reliable model. This indicates that the models predictions are highly correlated with the true outcomes. The model is effective at correctly classifying both positive and negative instances for each class. The high MCC value also confirms that the model's performance is not the result of chance or bias toward a particular class. Although the value of this metric is high, it too is lower than that of the classification tree, also indicating that the classification tree is the favorable model. 

\subsection{\textbf{Statistical Tests}}

To determine if a statistically significant difference exists between the performance of the classification tree and the k-NN classifier models, a comparative analysis was conducted using a 10-fold cross-validation procedure. The macro F1-score was used as the primary performance metric for this comparison, for reasons discussed above.

The results of the cross-validation show that the k-NN model achieved a significantly higher average performance. It yielded a mean macro F1-score of 0.8215, while the classification tree model achieved a mean macro F1-score of 0.8199.

To formally test the significance of the observed difference in the scores, a \textbf{Wilcoxon signed-rank test} was performed. This non-parametric statistical test is used to compare related samples, such as paired scores generated by a cross-validation procedure on the same data splits (note that the same test data was used for both of these models, but the data was pre-processed differently for each model). The test evaluates the null hypothesis, $H_0$, that there is no difference in the performance between the two models.

The test yielded a statistic of $W = 22$ and a p-value of $p = 0.6250$.

The p-value of 0.6250 is substantially greater than the conventional significance level of $\alpha = 0.05$. This means that the null hypothesis fails to be rejected.

This indicates that although the k-NN model shows a marginally higher mean F1-score, the difference in performance is not statistically significant. The small observed advantage for the k-NN model is most likely due to random chance and the specific splits used by the cross-validation process.

In conclusion, based on the statistical analysis, the classification tree and k-NN classifier exhibit comparable predictive performance on this dataset.

\section{\textbf{Conclusion}}

This report set out to compare the performance of two distinct machine learning algorithms, a classification tree, and a k-NN classifier, for the task of predicting forest cover types from the \texttt{forestCover.csv} dataset. The primary objective was to determine which model provided a more effective and practical solution, especially considering the various data quality issues present. The methodology involved targeted data preprocessing for each model, addressing class imbalance in the training data using SMOTE, systematic hyperparameter tuning via grid search with 5-fold cross-validation, and a comprehensive evaluation using a suite of performance metrics and statistical tests.

The investigation revealed that both the final classification tree and the k-NN classifier are highly capable models, achieving strong overall accuracies of 94\% and 93\%, respectively. A Wilcoxon signed-rank test was conducted to formally compare their performance, yielding a p-value of $p = 0.6250$. This result indicates that there is no statistically significant difference between the two models in terms of their predictive power on this dataset.

Despite the models having no statistically significant difference in performance, the final recommendation is to use the classification tree for this dataset and problem. This conclusion is based on several key factors. First, the decision tree consistently outperformed the k-NN model across several crucial evaluation metrics, including overall accuracy (94\% vs 93\%), Cohen's Kappa (0.8996 vs. 0.8870), and the Matthews Correlation Coefficient (0.8996 vs. 0.8873). It also demonstrated a better balance of precision and recall on the most challenging minority classes.

Second, the classification tree offers significant practical advantages. Its inherent structure provides a high degree of interpretability, allowing its decision making process to be scrutinized by domain experts. Furthermore, it is far less computationally expensive and requires significantly less data preprocessing. The decision tree's robustness to feature scaling, outliers, and correlated features stands in stark contrast to the extensive transformations required to make the k-NN model viable.

In conclusion, while both algorithms can accurately classify forest cover types, the classification tree emerges as the more effective and pragmatic choice. It delivers slightly better performance and greater insight with lower computational cost, making it the preferred solution for this problem.

\begin{thebibliography}{00}
\bibitem{b1} GeeksforGeeks, Pruning decision trees, {\em GeeksforGeeks}, 23 Jul, 2025. [Online]. Available: https://www.geeksforgeeks.org/machine-learning/pruning-decision-trees/. [Accessed: Aug. 20, 2025].
\bibitem{b2} P. Patidar, A. Tiwari, Handling Missing Value in Decision Tree Algorithm, {\em International Journal of Computer Applications}, vol. 70, no. 13, May 2013.
\end{thebibliography}
\vspace{12pt}


\end{document}
