\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{ML441 Assignment 2}

\author{\IEEEauthorblockN{RH Buhr, 26440873}
\IEEEauthorblockA{\textit{BDatSci Programme, 4th Year} \\
\textit{Stellenbosch University}\\
Stellenbosch, South Africa \\
26440873@sun.ac.za}
}

\maketitle

\begin{abstract}
This is the abstract.
\end{abstract}

% Keywords to give readers an idea of what paper is about.
% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\section{\textbf{Introduction}}
This is the intro of the report.

\section{\textbf{Background}}

The task of predicting a forest cover type from cartographic data is a classic example of a supervised learning problem, specifically a multi-class classification problem. The core idea is to train a model on a dataset containing observations (parcels of land) with known descriptive features and known response labels. The trained model should then be able to predict the correct label for new, unseen observations. Myriad machine learning algorithms exist to accomplish this task, each with unique methodologies and inductive biases. This report will focus on two distinct approaches, an information-based method and a similarity-based method. 

\subsection{\textbf{Machine learning algorithm description}}

\subsubsection{\textbf{Classification Tree}}

A classification tree is an information-based machine learning model, that forms a tree-like structure of human-readable rules.. Non-terminal nodes in this tree represent rules applied to descriptive features, and terminal nodes represent the response feature predictions. The algorithm learns to recursively split the data into increasingly pure subsets based on the feature values that provide the most information. The homogeneity (purity) measures often used by classification trees are entropy and the Gini index.

Entropy is an information theory concept that measures the level of disorder of uncertainty in a set. In the context of decision trees, it measures the impurity of a node. Entropy falls between [0, 1], and an entropy value of 0 signifies a perfectly pure node, where all observations are from the same class. An entropy value of 1 represents the hightest level of disorder, e.g. a 50/50 split in a binary classification problem. The classification tree algorithm selects the split that provides the highest information gain, which is the reduction in entropy after the split. The formula for entropy is: $Entropy = -\sum_{i=1}^{C} p_i \log p_i$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

An alternative measure is the Gini index. It is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it were randomly labeled according to the distribution of the labels in the set. The Gini index also falls in the range [0, 1], and a Gini index score of 0 represents a perfectly pure node, while a higher score indicates greater impurity. Just like entropy, the algorithm seeks to find rules that result in the largest decrease in Gini index. The formula for Gini index is: $Gini\ index = 1 - \sum_{i=1}^{C} (p_i)^2$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

To prevent overfitting, classification trees are simplified through a process called pruning, which reduces the tree's size by removing branches that contribute little to its predictive power. A tree that grows till every terminal node is perfectly pure often captures noise in the training data and generalizes poorly to unseen data. There are two main strategies to prune classification trees and achieve a better trade-off between bias and variance.

Pre-pruning stops the tree's growth before it is fully induced. This method involves setting stopping criteria that halt the splitting of nodes once they are met. Examples include:

\begin{itemize}
    \item Maximum Tree Depth: Limit longest path from root to leaf nodes.
    \item Minimum Samples for a Split: Require a node to have at least a certain amount of samples before it can be split.
    \item Minimum Samples per Leaf: Ensure that terminal nodes contain a minimum number of samples.
\end{itemize}

Pre-pruning is cost-effective because it stops the tree from becoming too large, but a potential drawback is that it might stop the growth too early. This is where post-pruning comes in. It allows the tree to grow to its maximum size and then systematically cuts back the branches that are least important. The algorithm prunes subtrees that provide weak predictive power, typically be evaluating the tree's performance on a separate validation set \cite{b1}.

Classification Trees are a good approach to solve the given problem. They are highly interpretable, require little data preprocessing, and capture non-linear relationships. However they are prone to overfitting and can create biased trees if some classes dominate.

\subsubsection{\textbf{k-Nearest Neighbors}}

k-Nearest neighbors is a similarity based machine learning model. k-NN is a non-parametric algorithm, that assumes that an observation is likely to be similar to the data points closest to it in feature space.

k-NN works by storing all training instances, when a new instance to be predicted is presented to the model the instance is compared to the training data to find the most similar instances. The label of this instance is then assigned the majority label in the set of its closest k neighbors. To find these k most similar points k-NN uses difference metrics such as Euclidean distance: $d(x_1, x_2) = \sqrt{\sum_{i=1}^{p} (x_1i - x_2i)^2}$, Manhattan distance: $d(x_1, x_2) = \sum_{i=1}^{p} |x_1i - x_2i|$, or Minkowski distance: $d(x_1, x_2) = (\sum_{i=1}^{p} |x_1i - x_2i|^q)^\frac{1}{q}$, to quantify the distance between observations, where $p$ is the number of descriptive features, and $q$ is a parameter.

Once the distances between the observation of interest and all other observations in the training set have been calculated, the algorithm can then select the k closest points (smallest distance), and assign a prediction to the label of the observation of interest based on a majority vote or a weighted majority vote.

The value of k is a hyperparameter that needs to be tuned via cross validation or another robust method. A low value for k will result in a model with high variance and low bias, potentially overfitting the training data. While a high value for k will result in a model with low variance and high bias, which will underfit the training data.

kNN is a robust and flexible non-parametric machine learning model, but it does have some drawbacks. It is extremely sensitive to feature scale. If one feature is orders of magnitude larger than the others then it will dominate in the distance calculations and the information provided by all other features would be disregarded. Hence it is always best to standardize all features before training a k-NN model.

\subsection{\textbf{Expectations wrt data quality issues}}

\subsubsection{\textbf{Classification Tree}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact classification tree models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Unlike other algorithms, that require missing values to be removed or imputed, some classification tree algorithms, like CART of C4.5, have elegant, built-in methods to handle missing values directly, \cite{b2}.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: The predictive accuracy of the model will not be affected. The tree induction algorithm will select one of the features for a split (which ever provides the most information gain), and the other will not be considered for subsequent splits as it does not provide much additional information. The only part of the model that is affected is the structure, since these two features are correlated the algorithm arbitrarily choses one of them to do the initial split on, if slightly different training data is used, the algorithm might choose the other variable.
    \item \textbf{\texttt{Inclination} contains only noisy values}:  A feature containing only noisy values will have very little negative impact on the final performance of a classification tree. Classification tree induction algorithms find the best feature and split-point that most effectively separates the data into distinct classes. A noisy feature has values that are essentially random wrt the target. This means that any split made on this feature will fail to create more informative subgroups. Since the tree building algorithm is greedy it will always look for more informative features and ignore \texttt{Inclination}. However if the noisy feature happens to be correlated with the target by chance then the algorithm might end including it in the tree leading to a more complex, overfit model.
    \item \textbf{Features with outliers}: Classification trees are robust to outliers, their structure and splitting criteria naturally isolate extreme values. Trees split based on thresholds that separate groups, a few extreme values rarely change where the best split is. Outliers get isolated into their own small leaf nodes that do not distort the main decision boundaries learned from the majority data. 
    \item \textbf{Features differing numeric ranges}: This will have no impact on a classification tree. Classification trees are completely insensitive to the scale of the features, they only consider thresholds within features, and do not consider feature scales. Entropy and the Gini index only care about the purity of resulting groups, not the magnitude or scale of the threshold itself.
    \item \textbf{Numerical and categorical features}: Decision trees can naturally handle both numerical and categorical features. They are non-parametric models that do nto rely on distance calculations which make them uniquely suited for datasets with mixed feature types. For numerical features, the algorithm finds an optimal threshold that best separates classes. For categorical features, the algorithm partitions the categories into two or more subsets that result in the purest subgroups.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one has no impact on the final classification tree model, the algorithm completely ignores it. Since \texttt{Water\_Level} is constant, it is impossible to split the data using it. This means this feature provides no information to the model and it will not aid in reducing impurity. The greedy nature of tree-based models ensure that they will never choose features that provide zero benefit.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: A feature with a unique value for each observation will have a severely negative impact on a classification tree, causing it to overfit the training data. Since decision tree algorithms are greedy a unique identifier presents itself as the perfect feature for splitting, because every value in the \texttt{Observation\_ID} column is unique, the algorithm can create a split for each individual observation resulting in leaf nodes each containing one data point. This will minimize impurity to the greatest extent, and the algorithm sees this as the ultimate success. Features with unique value for each observation will cause classification trees to severely overfit the training data, having 100\% train accuracy, but very poor generalization.
    \item \textbf{Skew class distribution}: Training a classification tree on a dataset with an extremely skewed response will create a biased model that performs poorly on the minority class. The easiest way for the tree to achieve high accuracy is to simply predict the majority class most of the time. Performance metrics like accuracy will show good performance, but minority class recall will be very poor.
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors Description}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact k-NN classification models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values directly disrupt the k-NN algorithm because it relies on distances between data points to make predictions. If a feature's value is missing the distance calculation cannot be completed and the model will fail.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: Correlated features can severely bias a k-NN model by giving disproportionate weight to the single underlying trait they represent, effectively counting its influence twice during the distance calculation. This distortion of feature space skews the identification of nearest neighbors, which can negatively impact the performance of the model.
    \item \textbf{\texttt{Inclination} contains only noisy values}: A feature containing only noisy values will negatively impact a k-NN classification model. Noisy features introduce randomness and irrelevant information, which can distort distance calculations making the model less accurate. The noisy feature adds random, meaningless fluctuations to the computed distances. This distortion might lead the algorithm to identify incorrect nearest neighbors, choosing neighbors closer in terms of the noisy feature, but farther away in terms of the meaningful features. Additionally including a noisy feature will lead to overfitting, since the model might try to learn the noise, which will lead to poor generalization.
    \item \textbf{Features with outliers}: Outliers can negatively impact a k-NN classifier because its distance calculation is highly sensitive to the magnitude of feature values. For instance, two data points might be very close to each other in several descriptive features, but an outlier in a single feature can create such a large distance in that one dimension that it disproportionately inflates the overall distance metric. This skew causes the algorithm to misidentify the true nearest neighbors, leading to inaccurate predictions.
    \item \textbf{Features differing numeric ranges}: Differing feature magnitudes have significant impact on k-NN based classification models. Features with larger numeric ranges will disproportionately dominate the distance calculations, overshadowing the contributions of features on smaller scales, regardless of predictive importance. This bias can severely skew the identification of nearest neighbors, leading to a reduction in the model's performance
    \item \textbf{Numerical and categorical features}: A k-NN classifier cannot be directly applied to a dataset with a mix of numerical and categorical features because its distance-based metrics are only applicable to numeric features, and undefined for categorical features. It is critical to preprocess the categorical features correctly to transform them into a numerical format. Care must be taken to distinguish between nominal and ordinal features as incorrect encoding can create artificial relationships that mislead the model.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one will have no impact on a k-NN classification model, because it provides no useful information to distinguish between data points. When the k-NN algorithm calculates distance between any two points, the difference between the \texttt{Water\_Level} feature between both points will be zero. It contributes nothing to the overall distance calculation and has no influence on the nearest neighbors selected.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Including a unique identifier is highly detrimental to a k-NN model, as it introduces data with no predictive value that can dominate the distance metric. This can cause the model to overfit by trying to learn arbitrary relationships between data points base on the observation ID rather than genuine patterns in the descriptive features, leading to negatively impacted model performance and poor generalization.
    \item \textbf{Skew class distribution}: A skewed class distribution significantly degrades k-NN classifier performance, as the algorithm's majority voting mechanism is inherently biased towards the dominant class. When one or two classes vastly outnumber the others, their distances are more likely to dominate any given neighborhood. Consequently the model will frequently misclassify minority class instances as the majority classes, leading to high accuracy for the majority class, but low recall for the minority class. This predictive bias becomes even more pronounced as the value of \textit{k} increases, as a larger neighborhood is more likely to reflect the imbalanced distribution.
\end{itemize}


\section{Implementation}
This is where I need to explain how sklearn is used.

\section{Empirical process}

\subsection{Data pre-processing}

\subsection{Control parameter tuning}

\subsection{Performance metric}

\subsection{Analysis process}

\section{Results \& discussion}

\section{Conclusions}

\begin{thebibliography}{00}
\bibitem{b1} GeeksforGeeks, Pruning decision trees, {\em GeeksforGeeks}, 23 Jul, 2025. [Online]. Available: https://www.geeksforgeeks.org/machine-learning/pruning-decision-trees/. [Accessed: Aug. 20, 2025].
\bibitem{b2} P. Patidar, A. Tiwari, Handling Missing Value in Decision Tree Algorithm, {\em International Journal of Computer Applications}, vol. 70, no. 13, May 2013.
\end{thebibliography}
\vspace{12pt}


\end{document}
