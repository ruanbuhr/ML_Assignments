\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{ML441 Assignment 2}

\author{\IEEEauthorblockN{RH Buhr, 26440873}
\IEEEauthorblockA{\textit{BDatSci Programme, 4th Year} \\
\textit{Stellenbosch University}\\
Stellenbosch, South Africa \\
26440873@sun.ac.za}
}

\maketitle

\begin{abstract}
This report presents a comparative analysis of the k-Nearest Neighbors (k-NN) and Decision Tree classification algorithms for predicting forest cover types using the \texttt{forestCover.csv} dataset. The primary goal is to determine which model offers superior performance when faced with the dataset's inherent data quality issues.

The investigation begins with a foundational overview of both the k-NN and classification tree models, highlighting their advantages and disadvantages in the context of this specific problem domain. The report details the critical preprocessing strategies implemented to overcome the dataset's limitations, ensuring robust model training. A systematic approach to control parameter tuning for each classifier is described, alongside the performance metric used for a fair evaluation.

% Main observation - subject to change.
The main observation from this analysis is that while both models are capable classifiers, the classification tree demonstrates a notable advantage in predictive accuracy and interpretability for this particular dataset.

This report provides a detailed walkthrough of the process followed to arrive at this conclusion, offering valuable insights in selecting between these fundamental classification models for complex applications.
\end{abstract}

\section{\textbf{Introduction}}
This is the intro of the report.

\section{\textbf{Background}}

The task of predicting a forest cover type from cartographic data is a classic example of a supervised learning problem, specifically a multi-class classification problem. The core idea is to train a model on a dataset containing observations (parcels of land) with known descriptive features and known response labels. The trained model should then be able to predict the correct label for new, unseen observations. Myriad machine learning algorithms exist to accomplish this task, each with unique methodologies and inductive biases. This report will focus on two distinct approaches, an information-based method and a similarity-based method. 

\subsection{\textbf{Machine learning algorithm description}}

\subsubsection{\textbf{Classification Tree}}

A classification tree is an information-based machine learning model, that forms a tree-like structure of human-readable rules.. Non-terminal nodes in this tree represent rules applied to descriptive features, and terminal nodes represent the response feature predictions. The algorithm learns to recursively split the data into increasingly pure subsets based on the feature values that provide the most information. The homogeneity (purity) measures often used by classification trees are entropy and the Gini index.

Entropy is an information theory concept that measures the level of disorder of uncertainty in a set. In the context of decision trees, it measures the impurity of a node. Entropy falls between [0, 1], and an entropy value of 0 signifies a perfectly pure node, where all observations are from the same class. An entropy value of 1 represents the hightest level of disorder, e.g. a 50/50 split in a binary classification problem. The classification tree algorithm selects the split that provides the highest information gain, which is the reduction in entropy after the split. The formula for entropy is: $Entropy = -\sum_{i=1}^{C} p_i \log p_i$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

An alternative measure is the Gini index. It is a measure of how often a randomly chosen element from a set would be incorrectly labeled if it were randomly labeled according to the distribution of the labels in the set. The Gini index also falls in the range [0, 1], and a Gini index score of 0 represents a perfectly pure node, while a higher score indicates greater impurity. Just like entropy, the algorithm seeks to find rules that result in the largest decrease in Gini index. The formula for Gini index is: $Gini\ index = 1 - \sum_{i=1}^{C} (p_i)^2$, where $C$ is the number of classes and $p_i$ is the probability of an element being in class $i$.

To prevent overfitting, classification trees are simplified through a process called pruning, which reduces the tree's size by removing branches that contribute little to its predictive power. A tree that grows till every terminal node is perfectly pure often captures noise in the training data and generalizes poorly to unseen data. There are two main strategies to prune classification trees and achieve a better trade-off between bias and variance.

Pre-pruning stops the tree's growth before it is fully induced. This method involves setting stopping criteria that halt the splitting of nodes once they are met. Examples include:

\begin{itemize}
    \item Maximum Tree Depth: Limit longest path from root to leaf nodes.
    \item Minimum Samples for a Split: Require a node to have at least a certain amount of samples before it can be split.
    \item Minimum Samples per Leaf: Ensure that terminal nodes contain a minimum number of samples.
\end{itemize}

Pre-pruning is cost-effective because it stops the tree from becoming too large, but a potential drawback is that it might stop the growth too early. This is where post-pruning comes in. It allows the tree to grow to its maximum size and then systematically cuts back the branches that are least important. The algorithm prunes subtrees that provide weak predictive power, typically be evaluating the tree's performance on a separate validation set \cite{b1}.

Classification Trees are a good approach to solve the given problem. They are highly interpretable, require little data preprocessing, and capture non-linear relationships. However they are prone to overfitting and can create biased trees if some classes dominate.

\subsubsection{\textbf{k-Nearest Neighbors}}

k-Nearest neighbors is a similarity based machine learning model. k-NN is a non-parametric algorithm, that assumes that an observation is likely to be similar to the data points closest to it in feature space.

k-NN works by storing all training instances, when a new instance to be predicted is presented to the model the instance is compared to the training data to find the most similar instances. The label of this instance is then assigned the majority label in the set of its closest k neighbors. To find these k most similar points k-NN uses difference metrics such as Euclidean distance: $d(x_1, x_2) = \sqrt{\sum_{i=1}^{p} (x_1i - x_2i)^2}$, Manhattan distance: $d(x_1, x_2) = \sum_{i=1}^{p} |x_1i - x_2i|$, or Minkowski distance: $d(x_1, x_2) = (\sum_{i=1}^{p} |x_1i - x_2i|^q)^\frac{1}{q}$, to quantify the distance between observations, where $p$ is the number of descriptive features, and $q$ is a parameter.

Once the distances between the observation of interest and all other observations in the training set have been calculated, the algorithm can then select the k closest points (smallest distance), and assign a prediction to the label of the observation of interest based on a majority vote or a weighted majority vote.

The value of k is a hyperparameter that needs to be tuned via cross validation or another robust method. A low value for k will result in a model with high variance and low bias, potentially overfitting the training data. While a high value for k will result in a model with low variance and high bias, which will underfit the training data.

kNN is a robust and flexible non-parametric machine learning model, but it does have some drawbacks. It is extremely sensitive to feature scale. If one feature is orders of magnitude larger than the others then it will dominate in the distance calculations and the information provided by all other features would be disregarded. Hence it is always best to standardize all features before training a k-NN model.

\subsection{\textbf{Expectations wrt data quality issues}}

\subsubsection{\textbf{Classification Tree}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact classification tree models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Unlike other algorithms, that require missing values to be removed or imputed, some classification tree algorithms, like CART of C4.5, have elegant, built-in methods to handle missing values directly, \cite{b2}.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: The predictive accuracy of the model will not be affected. The tree induction algorithm will select one of the features for a split (which ever provides the most information gain), and the other will not be considered for subsequent splits as it does not provide much additional information. The only part of the model that is affected is the structure, since these two features are correlated the algorithm arbitrarily choses one of them to do the initial split on, if slightly different training data is used, the algorithm might choose the other variable.
    \item \textbf{\texttt{Inclination} contains only noisy values}:  A feature containing only noisy values will have very little negative impact on the final performance of a classification tree. Classification tree induction algorithms find the best feature and split-point that most effectively separates the data into distinct classes. A noisy feature has values that are essentially random wrt the target. This means that any split made on this feature will fail to create more informative subgroups. Since the tree building algorithm is greedy it will always look for more informative features and ignore \texttt{Inclination}. However if the noisy feature happens to be correlated with the target by chance then the algorithm might end including it in the tree leading to a more complex, overfit model.
    \item \textbf{Features with outliers}: Classification trees are robust to outliers, their structure and splitting criteria naturally isolate extreme values. Trees split based on thresholds that separate groups, a few extreme values rarely change where the best split is. Outliers get isolated into their own small leaf nodes that do not distort the main decision boundaries learned from the majority data. 
    \item \textbf{Features with differing numeric ranges}: This will have no impact on a classification tree. Classification trees are completely insensitive to the scale of the features, they only consider thresholds within features, and do not consider feature scales. Entropy and the Gini index only care about the purity of resulting groups, not the magnitude or scale of the threshold itself.
    \item \textbf{Numerical and categorical features}: Decision trees can naturally handle both numerical and categorical features. They are non-parametric models that do nto rely on distance calculations which make them uniquely suited for datasets with mixed feature types. For numerical features, the algorithm finds an optimal threshold that best separates classes. For categorical features, the algorithm partitions the categories into two or more subsets that result in the purest subgroups.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one has no impact on the final classification tree model, the algorithm completely ignores it. Since \texttt{Water\_Level} is constant, it is impossible to split the data using it. This means this feature provides no information to the model and it will not aid in reducing impurity. The greedy nature of tree-based models ensure that they will never choose features that provide zero benefit.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: A feature with a unique value for each observation will have a severely negative impact on a classification tree, causing it to overfit the training data. Since decision tree algorithms are greedy a unique identifier presents itself as the perfect feature for splitting, because every value in the \texttt{Observation\_ID} column is unique, the algorithm can create a split for each individual observation resulting in leaf nodes each containing one data point. This will minimize impurity to the greatest extent, and the algorithm sees this as the ultimate success. Features with unique value for each observation will cause classification trees to severely overfit the training data, having 100\% train accuracy, but very poor generalization.
    \item \textbf{Skew class distribution}: Training a classification tree on a dataset with an extremely skewed response will create a biased model that performs poorly on the minority class. The easiest way for the tree to achieve high accuracy is to simply predict the majority class most of the time. Performance metrics like accuracy will show good performance, but minority class recall will be very poor.
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors}}

The remaining data quality issues present in the \texttt{forestCover.csv} dataset will impact k-NN classification models in the following ways:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values directly disrupt the k-NN algorithm because it relies on distances between data points to make predictions. If a feature's value is missing the distance calculation cannot be completed and the model will fail.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: Correlated features can severely bias a k-NN model by giving disproportionate weight to the single underlying trait they represent, effectively counting its influence twice during the distance calculation. This distortion of feature space skews the identification of nearest neighbors, which can negatively impact the performance of the model.
    \item \textbf{\texttt{Inclination} contains only noisy values}: A feature containing only noisy values will negatively impact a k-NN classification model. Noisy features introduce randomness and irrelevant information, which can distort distance calculations making the model less accurate. The noisy feature adds random, meaningless fluctuations to the computed distances. This distortion might lead the algorithm to identify incorrect nearest neighbors, choosing neighbors closer in terms of the noisy feature, but farther away in terms of the meaningful features. Additionally including a noisy feature will lead to overfitting, since the model might try to learn the noise, which will lead to poor generalization.
    \item \textbf{Features with outliers}: Outliers can negatively impact a k-NN classifier because its distance calculation is highly sensitive to the magnitude of feature values. For instance, two data points might be very close to each other in several descriptive features, but an outlier in a single feature can create such a large distance in that one dimension that it disproportionately inflates the overall distance metric. This skew causes the algorithm to misidentify the true nearest neighbors, leading to inaccurate predictions.
    \item \textbf{Features with differing numeric ranges}: Differing feature magnitudes have significant impact on k-NN based classification models. Features with larger numeric ranges will disproportionately dominate the distance calculations, overshadowing the contributions of features on smaller scales, regardless of predictive importance. This bias can severely skew the identification of nearest neighbors, leading to a reduction in the model's performance
    \item \textbf{Numerical and categorical features}: A k-NN classifier cannot be directly applied to a dataset with a mix of numerical and categorical features because its distance-based metrics are only applicable to numeric features, and undefined for categorical features. It is critical to preprocess the categorical features correctly to transform them into a numerical format. Care must be taken to distinguish between nominal and ordinal features as incorrect encoding can create artificial relationships that mislead the model.
    \item \textbf{\texttt{Water\_Level} has a cardinality of one}: A feature with a cardinality of one will have no impact on a k-NN classification model, because it provides no useful information to distinguish between data points. When the k-NN algorithm calculates distance between any two points, the difference between the \texttt{Water\_Level} feature between both points will be zero. It contributes nothing to the overall distance calculation and has no influence on the nearest neighbors selected.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Including a unique identifier is highly detrimental to a k-NN model, as it introduces data with no predictive value that can dominate the distance metric. This can cause the model to overfit by trying to learn arbitrary relationships between data points base on the observation ID rather than genuine patterns in the descriptive features, leading to negatively impacted model performance and poor generalization.
    \item \textbf{Skew class distribution}: A skewed class distribution significantly degrades k-NN classifier performance, as the algorithm's majority voting mechanism is inherently biased towards the dominant class. When one or two classes vastly outnumber the others, their distances are more likely to dominate any given neighborhood. Consequently the model will frequently misclassify minority class instances as the majority classes, leading to high accuracy for the majority class, but low recall for the minority class. This predictive bias becomes even more pronounced as the value of \textit{k} increases, as a larger neighborhood is more likely to reflect the imbalanced distribution.
\end{itemize}


\section{\textbf{Implementation}}

The primary goal is to classify forest cover types by implementing, tuning, and comparing two distinct classification algorithms, decision tree and k-NN classifiers.

To achieve this goal, Python with Pandas, and Scikit-Learn was used to preprocess the given \texttt{forestCover.csv} dataset and train both a classification tree and a k-NN classifier on the cleaned data.

The stages of the implementation process for each model consisted of: 

\begin{itemize}
    \item Data preprocessing
    \item Addressing class imbalance in training set
    \item Systematic hyperparameter tuning using grid search with cross-validation
    \item Final model evaluation on unseen test data
\end{itemize}

During the data preprocessing stage, the data was prepared differently for the decision tree than the k-NN model, this required the use of two different copies of the original dataset. Since the decision tree is more robust to the issues that are mentioned to still be present in the dataset, less preprocessing was required. The dataset to be used for k-NN model training required substantially more preprocessing as k-NN models, being distance based models, are more sensitive to the issues present in the dataset.

After the data quality issues in the descriptive features for both datasets were handled accordingly, the class imbalance in the response, \texttt{Cover\_Type}, was addressed by applying SMOTE. This balanced the dataset by over sampling the minority classes (3, 4, 5, 6, 7) to create a more balanced dataset. It is important to note that SMOTE was applied only to the training datasets after both datasets were were train-test split to prevent data leakage and ensure unbiased evaluation of the model's performance on unseen data.

After applying SMOTE, the models were trained and hyperparameter tuning was done. For both models grid search with 5-fold cross validation was used. Grid search is a hyper parameter tuning approach where a user provides a set of all values the different model parameters are allowed to take on, the algorithm then evaluates and compares all the different combinations of hyperparameter values in order to find the best one. 5-Fold cross validation was used because it strikes a perfect balance between speed and stability. It works by splitting the dataset into five equal parts, and for each round (five total) the algorithm uses four folds for training and one for testing. The test fold is rotated each round, and the five test scores are averaged in the end to get a final model performance estimate.

Performance metric used to score the models during cross validation was F1 Macro. The macro-averaged F1 score calculates the F1 score for each class independently and then finds their unweighted average. This means that it treats all classes as equally important, regardless of how many samples are in the dataset. Even though SMOTE forces a model to pay attention to underrepresented classes during training the goal is not just to have a balanced training set, it is to build a model that performs well on the real-world imbalanced data. Using F1 Macro as scoring metric causes the cross validation algorithm to consider all categories equally important, including those that were originally in the minority.

After cross validation yielded the best classification tree and k-NN classifier, the models were evaluated by analyzing their accuracy, precision, recall, and F1 score which is critical for understanding model performance in a multi-class problem.

\section{\textbf{Empirical process}}

\subsection{\textbf{Data pre-processing}}

\subsubsection{\textbf{Classification Tree}}

This subsection details the data pre-processing steps that were taken to train the classification tree model. The following list mentions the data quality issues and the preprocessing steps followed:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values were dropped. The observations with missing values were found to make up only 0.05\% of the dataset, meaning removing them will not bias the dataset. Even though some classification tree algorithms are robust to missing values, SMOTE is not, and as discussed later, it is critical that SMOTE be applied to the training dataset.
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: Nothing was done. As mentioned previously, correlated features do not have any effect on decision trees, because the information gain by the underlying feature is only captured in one split. The other variable will not provide any additional information and will not be considered for future splits.
    \item \textbf{\texttt{Inclination} contains noisy values}: Nothing was done. Classification tree induction splits data based on information gain. A noisy feature provides no information gain, hence the algorithm will just ignore features that contain noisy values.
    \item \textbf{Features with outliers}: Nothing was done. Outliers do not affect the thresholds decided upon by decision trees, because a few extreme values rarely change where the best split is.
    \item \textbf{Features with differing numeric ranges}: Nothing was done as classification trees are completely insensitive to the scale of features since they only consider splits/thresholds within features. The purity metrics only care about the purity of resulting subgroups, not the magnitude of the values.
    \item \textbf{Numerical and categorical features}: Convert categorical feature to numeric. Even though classification trees can naturally handle numeric and categorical features, SMOTE can not, hence the only categorical feature \texttt{Soil\_Type1} needed to be converted to numeric. All occurrences of \textit{positive} were assigned a value of \textit{0}, and all occurrences of \textit{negative} were assigned a value of \textit{1}. This keeps the \texttt{Soil\_Type} one-hot encoded group mutually exclusive.
    \item \textbf{\texttt{Water\_Level} has cardinality of one}: Nothing was done. Constant features are ignored by the tree induction algorithm. Since the feature is constant, there is no way for the algorithm to split the data using this feature, hence it provides no information gain and is just ignored.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Removed \texttt{Observation\_ID} as a descriptive feature. Features with unique values for each observation are detrimental to the performance of decision trees, as they provide maximal information gain to split on. Splitting on these features will cause each observation to be in it's own leaf node, leading to severe overfitting. To combat this, \texttt{Observation\_ID} was removed as a descriptive feature. When using pandas to load the dataset, \texttt{df = pd.read\_csv('data/forestCover.csv', index\_col='Observation\_ID', na\_values='?')}, \texttt{Observation\_ID} is set to the index to prevent it from being used as a descriptive feature.
    \item \textbf{Skew class distribution}: SMOTE was applied to the training dataset to oversample the minority classes and create synthetic observations so the model can more precisely learn the patterns present in the minority classes, instead of the majority classes dominating. 
\end{itemize}

\subsubsection{\textbf{k-Nearest Neighbors}}

This subsection details the data pre-processing steps that were taken to train the k-NN classifier model. The following list mentions the data quality issues and the preprocessing steps followed:

\begin{itemize}
    \item \textbf{Missing Values}: Missing values were dropped. As mentioned above the observations with missing values were found to make up only 0.05\% of the observations, meaning removing them will not bias the dataset. k-NN is a distance based algorithm, its distance calculations are only defined for numeric values and will not work for missing values. Also SMOTE is not robust to missing values. 
    \item \textbf{\texttt{Facet} correlated with \texttt{Aspect}}: \texttt{Aspect} was dropped from the dataset. Correlated features can severely distort k-NN ability to find true nearest neighbors, because the underlying effect is counted twice, which skews the k-NN distance metrics.
    \item \textbf{\texttt{Inclination} contains noisy values}: \texttt{Inclination} was dropped from the dataset. Noisy features contain random fluctuations which distort the k-NN distance metrics making them unable to find the true nearest neighbors. Removing noisy features is the best course of action.
    \item \textbf{Features with outliers}: A $log(1 + x)$ transformation was applied to the feature \texttt{Horizontal\_Distance\_To\_Hydrology} and afterward Scikit-Learn \texttt{RobustScaler} was applied to all features. The $log(1 + x)$ was applied to \texttt{Horizontal\_Distance\_To\_Hydrology}, because it is the only feature with outliers that severely skew its distribution. It was decided to keep these outliers, because they represent observations of forest data that are not close to a water source in horizontal distance and removing them would lead to a loss in information. The $log(1 + x)$ transformation keeps all the information in the feature while drastically reducing the distance between inlier and outlier data points. After the transformation was applied Scikit-Learn \texttt{RobustScaler} was applied to all features. It scales features using statistics that are immune to outliers, like the median and interquartile range. This keeps all information present in the features while reducing the effects of outliers.
    \item \textbf{Features with differing numeric ranges}: Scikit-Learn \texttt{RobustScaler} was applied to all features. Differing numerical ranges can greatly skew a k-NN models ability to find the true nearest neighbors, as features with larger magnitudes dominate over smaller features in the distance calculations. The \texttt{RobustScaler} was used because, as discussed above, it scales data in a way that mitigates the effect of outliers while ensuring every feature contributes fairly to the distance metric.
    \item \textbf{Numerical and categorical features}: Convert categorical features into numeric. k-NN distance metrics are only defined for numerical data and do not work on categorical data, so categorical features need to be converted to numerical features. The only categorical feature \texttt{Soil\_Type1} needed to be converted to numeric. All occurrences of \textit{positive} were assigned a value of \textit{0}, and all occurrences of \textit{negative} were assigned a value of \textit{1}. This ensures k-NN distance metrics are defined for this feature.
    \item \textbf{\texttt{Water\_Level} has cardinality of one}: Nothing was done. When k-NN calculates distance between two points, the distance between the \texttt{Water\_Level} component will always be zero because it is a constant feature, this means it has no effect on the model's ability to find the nearest neighbors.
    \item \textbf{\texttt{Observation\_ID} has a unique value for each observation}: Removed \texttt[Observation\_ID] as a descriptive feature. Including a feature with a unique value for each observation provides no predictive information while adding data that can skew the distance metric and interfere with a k-NN model's ability to find nearest neighbors. To combat this, \texttt{Observation\_ID} was removed as a descriptive feature. When using pandas to load the dataset, \texttt{df = pd.read\_csv('data/forestCover.csv', index\_col='Observation\_ID', na\_values='?')}, \texttt{Observation\_ID} is set to the index to prevent it from being used as a descriptive feature.
    \item \textbf{Skew class distribution}: SMOTE was applied to the training dataset to oversample the minority classes and create synthetic observations so the model can more precisely learn the patterns present in the minority classes, instead of the majority classes dominating. 
\end{itemize}

\subsection{\textbf{Control parameter tuning}}

\subsection{Performance metric}

\subsection{Analysis process}

\section{Results \& discussion}

\section{Conclusions}

\begin{thebibliography}{00}
\bibitem{b1} GeeksforGeeks, Pruning decision trees, {\em GeeksforGeeks}, 23 Jul, 2025. [Online]. Available: https://www.geeksforgeeks.org/machine-learning/pruning-decision-trees/. [Accessed: Aug. 20, 2025].
\bibitem{b2} P. Patidar, A. Tiwari, Handling Missing Value in Decision Tree Algorithm, {\em International Journal of Computer Applications}, vol. 70, no. 13, May 2013.
\end{thebibliography}
\vspace{12pt}


\end{document}
