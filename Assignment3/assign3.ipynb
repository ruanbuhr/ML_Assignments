{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65b09c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 12:53:03.773832: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-24 12:53:03.794410: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-24 12:53:04.506885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-24 12:53:05.917149: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-24 12:53:05.922055: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c5eb9",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6fa024",
   "metadata": {},
   "source": [
    "The base Tensorflow Keras RNN is an Elman RNN. All that is needed is to code a Jordan and Multi-RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JordanRNNCell(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "    def __init__(self, units, output_dim, activation='tanh', output_activation=None, kernel_init='glorot_uniform', recurrent_init='glorot_uniform', bias_init='zeros', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.output_activation = tf.keras.activations.get(output_activation)\n",
    "        self.kernel_init = tf.keras.initializers.get(kernel_init)\n",
    "        self.recurrent_init = tf.keras.initializers.get(recurrent_init)\n",
    "        self.bias_init = tf.keras.initializers.get(bias_init)\n",
    "\n",
    "        self.state_size = output_dim\n",
    "        self.output_size = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.Wx = self.add_weight(shape=(input_dim, self.units), initializer=self.kernel_init, name='Wx')\n",
    "        self.Wy = self.add_weight(shape=(self.output_dim, self.units), initializer=self.recurrent_init, name='Wy')\n",
    "        self.bh = self.add_weight(shape=(self.units, ), initializer=self.bias_init, name='bh')\n",
    "\n",
    "        self.Wo = self.add_weight(shape=(self.units, self.output_dim), initializer=self.kernel_init, name='Wo')\n",
    "        self.bo = self.add_weight(shape=(self.output_dim, ), initializer=self.bias_init, name='bo')\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        y_prev = states[0]\n",
    "\n",
    "        h_t = tf.matmul(inputs, self.Wx) + tf.matmul(y_prev, self.Wy) + self.bh\n",
    "        h_t = self.activation(h_t)\n",
    "\n",
    "        y_t = tf.matmul(h_t, self.Wo) + self.bo\n",
    "\n",
    "        if self.output_activation is not None:\n",
    "            y_t = self.output_activation(y_t)\n",
    "        \n",
    "        return y_t, [y_t]\n",
    "    \n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        base.update({\n",
    "            \"units\": self.units,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation),\n",
    "            \"output_activation\": tf.keras.activations.serialize(self.output_activation),\n",
    "            \"kernel_initializer\": tf.keras.initializers.serialize(self.kernel_init),\n",
    "            \"recurrent_initializer\": tf.keras.initializers.serialize(self.recurrent_init),\n",
    "            \"bias_initializer\": tf.keras.initializers.serialize(self.bias_init),\n",
    "        })\n",
    "        return base\n",
    "\n",
    "\n",
    "def JordanRNN(units, output_dim, return_sequences=False, **kwargs):\n",
    "    cell = JordanRNNCell(units=units, output_dim=output_dim, **kwargs)\n",
    "    return tf.keras.layers.RNN(cell, return_sequences=return_sequences)\n",
    "\n",
    "\n",
    "def build_jordan_model(input_dim, units=64, output_dim=1, return_sequences=False, output_activation=None):\n",
    "    inp = tf.keras.Input(shape=(None, input_dim))\n",
    "    x = JordanRNN(units=units, output_dim=output_dim, return_sequences=return_sequences, activation='tanh', output_activation=output_activation)(inp)\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560915c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRNNCell(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "    def __init__(self, units, output_dim, activation='tanh', output_activation=None, kernel_init='glorot_uniform', recurrent_init='glorot_uniform', bias_init='zeros', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.output_activation = tf.keras.activations.get(output_activation)\n",
    "        self.kernel_init = tf.keras.initializers.get(kernel_init)\n",
    "        self.recurrent_init = tf.keras.initializers.get(recurrent_init)\n",
    "        self.bias_init = tf.keras.initializers.get(bias_init)\n",
    "\n",
    "        self.state_size = [units, output_dim]\n",
    "        self.output_size = output_dim\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.Wx = self.add_weight(shape=(input_dim, self.units), initializer=self.kernel_init, name='Wx')\n",
    "        self.Wh = self.add_weight(shape=(self.units, self.units), initializer=self.recurrent_init, name='Wh')\n",
    "        self.Wy = self.add_weight(shape=(self.output_dim, self.units), initializer=self.recurrent_init, name='Wy')\n",
    "        self.bh = self.add_weight(shape=(self.units, ), initializer=self.bias_init, name='bh')\n",
    "\n",
    "        self.Wo = self.add_weight(shape=(self.units, self.output_dim), initializer=self.kernel_init, name='Wo')\n",
    "        self.bo = self.add_weight(shape=(self.output_dim, ), initializer=self.bias_init, name='bo')\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_prev, y_prev = states\n",
    "\n",
    "        # takes both\n",
    "        h_t = tf.matmul(inputs, self.Wx) + tf.matmul(h_prev, self.Wh) + tf.matmul(y_prev, self.Wy) + self.bh\n",
    "        h_t = self.activation(h_t)\n",
    "\n",
    "        y_t = tf.matmul(h_t, self.Wo) + self.bo\n",
    "\n",
    "        if self.output_activation is not None:\n",
    "            y_t = self.output_activation(y_t)\n",
    "        \n",
    "        return y_t, [h_t, y_t]\n",
    "    \n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        base.update({\n",
    "            \"units\": self.units,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation),\n",
    "            \"output_activation\": tf.keras.activations.serialize(self.output_activation),\n",
    "            \"kernel_initializer\": tf.keras.initializers.serialize(self.kernel_init),\n",
    "            \"recurrent_initializer\": tf.keras.initializers.serialize(self.recurrent_init),\n",
    "            \"bias_initializer\": tf.keras.initializers.serialize(self.bias_init),\n",
    "        })\n",
    "        return base\n",
    "\n",
    "    \n",
    "def MultiRNN(units, output_dim, return_sequences=False, **kwargs):\n",
    "    cell = MultiRNNCell(units=units, output_dim=output_dim, **kwargs)\n",
    "    return tf.keras.layers.RNN(cell, return_sequences=return_sequences)\n",
    "\n",
    "\n",
    "def build_multi_model(input_dim, units=64, output_dim=1, return_sequences=False, output_activation=None):\n",
    "    inp = tf.keras.Input(shape=(None, input_dim))\n",
    "    x = MultiRNN(units=units, output_dim=output_dim, return_sequences=return_sequences, activation='tanh', output_activation=output_activation)(inp)\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
